---
title: "Supported Models"
description: "Choose your favorite LLM"
icon: "microchip-ai"

---

### web agent [example](https://github.com/web-agent/web-agent/blob/main/examples/models/web_agent_llm.py)

`Chatwebagent()` is our optimized in-house model, matching the accuracy of top models while completing tasks **3-5x** faster. [See our blog postâ†’](https://web-agent.com/posts/speed-matters)

```python
from web_agent import Agent, Chatwebagent

# Initialize the model (defaults to bu-latest)
llm = Chatwebagent()

# Or use the premium model
llm = Chatwebagent(model='bu-2-0')

# Create agent with the model
agent = Agent(
    task="...", # Your task here
    llm=llm
)
```

Required environment variables:

```bash .env
web_agent_API_KEY=
```

Get your API key from the [web agent Cloud](https://cloud.web-agent.com/new-api-key). New signups get \$10 free credit via OAuth or \$1 via email.

#### Available Models

- `bu-latest` or `bu-1-0`: Default model
- `bu-2-0`: Latest premium model with improved capabilities

#### Pricing

Chatwebagent offers competitive pricing per 1 million tokens:

**bu-1-0 / bu-latest (Default)**

| Token Type | Price per 1M tokens |
|------------|---------------------|
| Input tokens | $0.20 |
| Cached tokens | $0.02 |
| Output tokens | $2.00 |

**bu-2-0 (Premium)**

| Token Type | Price per 1M tokens |
|------------|---------------------|
| Input tokens | $0.60 |
| Cached tokens | $0.06 |
| Output tokens | $3.50 |


### Google Gemini [example](https://github.com/web-agent/web-agent/blob/main/examples/models/gemini.py)

<Warning>
`GEMINI_API_KEY` is deprecated and should be named `GOOGLE_API_KEY` as of 2025-05.
</Warning>

```python
from web_agent import Agent, ChatGoogle
from dotenv import load_dotenv

# Read GOOGLE_API_KEY into env
load_dotenv()

# Initialize the model
llm = ChatGoogle(model='gemini-flash-latest')

# Create agent with the model
agent = Agent(
    task="Your task here",
    llm=llm
)
```

Required environment variables:

```bash .env
GOOGLE_API_KEY=
```


### OpenAI [example](https://github.com/web-agent/web-agent/blob/main/examples/models/gpt-4.1.py)

`O3` model is recommended for best accuracy.

```python
from web_agent import Agent, ChatOpenAI

# Initialize the model
llm = ChatOpenAI(
    model="o3",
)

# Create agent with the model
agent = Agent(
    task="...", # Your task here
    llm=llm
)
```

Required environment variables:

```bash .env
OPENAI_API_KEY=
```

<Info>
  You can use any OpenAI compatible model by passing the model name to the
  `ChatOpenAI` class using a custom URL (or any other parameter that would go
  into the normal OpenAI API call).
</Info>

### Anthropic [example](https://github.com/web-agent/web-agent/blob/main/examples/models/claude-4-sonnet.py)

```python
from web_agent import Agent, ChatAnthropic

# Initialize the model
llm = ChatAnthropic(
    model="claude-sonnet-4-0",
)

# Create agent with the model
agent = Agent(
    task="...", # Your task here
    llm=llm
)
```

And add the variable:

```bash .env
ANTHROPIC_API_KEY=
```

### Azure OpenAI [example](https://github.com/web-agent/web-agent/blob/main/examples/models/azure_openai.py)

```python
from web_agent import Agent, ChatAzureOpenAI
from pydantic import SecretStr
import os

# Initialize the model
llm = ChatAzureOpenAI(
    model="o4-mini",
)

# Create agent with the model
agent = Agent(
    task="...", # Your task here
    llm=llm
)
```

Required environment variables:

```bash .env
AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com/
AZURE_OPENAI_API_KEY=
```

#### Using the Responses API (for GPT-5.1 Codex models)

<Info>
Azure OpenAI now requires `api_version >= 2025-03-01-preview` for certain models like `gpt-5.1-codex-mini`.
These models only support the [Responses API](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/responses) instead of the Chat Completions API.
</Info>

web agent automatically detects and uses the Responses API for these models:
- `gpt-5.1-codex`, `gpt-5.1-codex-mini`, `gpt-5.1-codex-max`
- `gpt-5-codex`, `codex-mini-latest`
- `computer-use-preview`

```python
from web_agent import Agent, ChatAzureOpenAI

# Auto-detection (recommended) - uses Responses API for gpt-5.1-codex-mini
llm = ChatAzureOpenAI(
    model="gpt-5.1-codex-mini",
    api_version="2025-03-01-preview",  # Required for Responses API
)

# Or explicitly enable/disable Responses API for any model
llm = ChatAzureOpenAI(
    model="gpt-4o",
    api_version="2025-03-01-preview",
    use_responses_api=True,  # Force Responses API (True/False/'auto')
)

agent = Agent(
    task="...",
    llm=llm
)
```

The `use_responses_api` parameter accepts:
- `'auto'` (default): Automatically uses Responses API for models that require it
- `True`: Force use of the Responses API
- `False`: Force use of the Chat Completions API

### AWS Bedrock [example](https://github.com/web-agent/web-agent/blob/main/examples/models/aws.py)

AWS Bedrock provides access to multiple model providers through a single API. We support both a general AWS Bedrock client and provider-specific convenience classes.

#### General AWS Bedrock (supports all providers)

```python
from web_agent import Agent, ChatAWSBedrock

# Works with any Bedrock model (Anthropic, Meta, AI21, etc.)
llm = ChatAWSBedrock(
    model="anthropic.claude-3-5-sonnet-20240620-v1:0",  # or any Bedrock model
    aws_region="us-east-1",
)

# Create agent with the model
agent = Agent(
    task="Your task here",
    llm=llm
)
```

#### Anthropic Claude via AWS Bedrock (convenience class)

```python
from web_agent import Agent, ChatAnthropicBedrock

# Anthropic-specific class with Claude defaults
llm = ChatAnthropicBedrock(
    model="anthropic.claude-3-5-sonnet-20240620-v1:0",
    aws_region="us-east-1",
)

# Create agent with the model
agent = Agent(
    task="Your task here",
    llm=llm
)
```

#### AWS Authentication

Required environment variables:

```bash .env
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_DEFAULT_REGION=us-east-1
```

You can also use AWS profiles or IAM roles instead of environment variables. The implementation supports:

- Environment variables (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_DEFAULT_REGION`)
- AWS profiles and credential files
- IAM roles (when running on EC2)
- Session tokens for temporary credentials
- AWS SSO authentication (`aws_sso_auth=True`)

## Groq [example](https://github.com/web-agent/web-agent/blob/main/examples/models/llama4-groq.py)

```python
from web_agent import Agent, ChatGroq

llm = ChatGroq(model="meta-llama/llama-4-maverick-17b-128e-instruct")

agent = Agent(
    task="Your task here",
    llm=llm
)
```

Required environment variables:

```bash .env
GROQ_API_KEY=
```

## Oracle Cloud Infrastructure (OCI) [example](https://github.com/web-agent/web-agent/blob/main/examples/models/oci_models.py)

OCI provides access to various generative AI models including Meta Llama, Cohere, and other providers through their Generative AI service.

```python
from web_agent import Agent, ChatOCIRaw

# Initialize the OCI model
llm = ChatOCIRaw(
    model_id="ocid1.generativeaimodel.oc1.us-chicago-1.amaaaaaask7dceya...",
    service_endpoint="https://inference.generativeai.us-chicago-1.oci.oraclecloud.com",
    compartment_id="ocid1.tenancy.oc1..aaaaaaaayeiis5uk2nuubznrekd...",
    provider="meta",  # or "cohere"
    temperature=0.7,
    max_tokens=800,
    top_p=0.9,
    auth_type="API_KEY",
    auth_profile="DEFAULT"
)

# Create agent with the model
agent = Agent(
    task="Your task here",
    llm=llm
)
```

Required setup:
1. Set up OCI configuration file at `~/.oci/config`
2. Have access to OCI Generative AI models in your tenancy
3. Install the OCI Python SDK: `uv add oci` or `pip install oci`

Authentication methods supported:
- `API_KEY`: Uses API key authentication (default)
- `INSTANCE_PRINCIPAL`: Uses instance principal authentication
- `RESOURCE_PRINCIPAL`: Uses resource principal authentication

## Ollama

1. Install Ollama: https://github.com/ollama/ollama
2. Run `ollama serve` to start the server
3. In a new terminal, install the model you want to use: `ollama pull llama3.1:8b` (this has 4.9GB)

```python
from web_agent import Agent, ChatOllama

llm = ChatOllama(model="llama3.1:8b")
```

## Langchain

[Example](https://github.com/web-agent/web-agent/blob/main/examples/models/langchain) on how to use Langchain with web agent.

## Qwen [example](https://github.com/web-agent/web-agent/blob/main/examples/models/qwen.py)

Currently, only `qwen-vl-max` is recommended for web agent. Other Qwen models, including `qwen-max`, have issues with the action schema format.
Smaller Qwen models may return incorrect action schema formats (e.g., `actions: [{"navigate": "google.com"}]` instead of `[{"navigate": {"url": "google.com"}}]`). If you want to use other models, add concrete examples of the correct action format to your prompt.

```python
from web_agent import Agent, ChatOpenAI
from dotenv import load_dotenv
import os

load_dotenv()

# Get API key from https://modelstudio.console.alibabacloud.com/?tab=playground#/api-key
api_key = os.getenv('ALIBABA_CLOUD')
base_url = 'https://dashscope-intl.aliyuncs.com/compatible-mode/v1'

llm = ChatOpenAI(model='qwen-vl-max', api_key=api_key, base_url=base_url)

agent = Agent(
    task="Your task here",
    llm=llm,
    use_vision=True
)
```

Required environment variables:

```bash .env
ALIBABA_CLOUD=
```

## ModelScope [example](https://github.com/web-agent/web-agent/blob/main/examples/models/modelscope_example.py)

```python
from web_agent import Agent, ChatOpenAI
from dotenv import load_dotenv
import os

load_dotenv()

# Get API key from https://www.modelscope.cn/docs/model-service/API-Inference/intro
api_key = os.getenv('MODELSCOPE_API_KEY')
base_url = 'https://api-inference.modelscope.cn/v1/'

llm = ChatOpenAI(model='Qwen/Qwen2.5-VL-72B-Instruct', api_key=api_key, base_url=base_url)

agent = Agent(
    task="Your task here",
    llm=llm,
    use_vision=True
)
```

Required environment variables:

```bash .env
MODELSCOPE_API_KEY=
```

### Vercel AI Gateway [example](https://github.com/web-agent/web-agent/blob/main/examples/models/vercel_ai_gateway.py)

Vercel AI Gateway provides an OpenAI-compatible API endpoint that acts as a proxy to various AI providers, with features like rate limiting, caching, and monitoring.

To see all available models, visit: https://ai-gateway.vercel.sh/v1/models

```python
from web_agent import Agent, ChatVercel
from dotenv import load_dotenv
import os

load_dotenv()

# Get API key (https://vercel.com/ai-gateway)
api_key = os.getenv('VERCEL_API_KEY')
if not api_key:
    raise ValueError('VERCEL_API_KEY is not set')

# Basic usage
llm = ChatVercel(
    model='openai/gpt-4o',
    api_key=api_key,
)

# With provider options - control which providers are used and in what order
# This will try Vertex AI first, then fall back to Anthropic if Vertex fails
llm_with_provider_options = ChatVercel(
    model='anthropic/claude-sonnet-4',
    api_key=api_key,
    provider_options={
        'gateway': {
            'order': ['vertex', 'anthropic']  # Try Vertex AI first, then Anthropic
        }
    },
)

agent = Agent(
    task="Your task here",
    llm=llm
)
```

Required environment variables:

```bash .env
VERCEL_API_KEY=
```

## Other models (DeepSeek, Novita, X...)

We support all other models that can be called via OpenAI compatible API. We are open to PRs for more providers.

**Examples available:**
- [DeepSeek](https://github.com/web-agent/web-agent/blob/main/examples/models/deepseek-chat.py)
- [Novita](https://github.com/web-agent/web-agent/blob/main/examples/models/novita.py)
- [OpenRouter](https://github.com/web-agent/web-agent/blob/main/examples/models/openrouter.py)
